# IA_P3_CIFAR10_PerezRegueiroMiguel

# CIFAR-10 CNN ‚Äì Pr√°ctica 3  
> Autor: TuNombre Apellido ‚Äì Curso 2025/26

[![Python 3.10](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow 2.x](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://tensorflow.org)

Repositorio reproducible de la pr√°ctica **‚ÄúVisi√≥n profunda con CNN en CIFAR-10‚Äù**.  
Incluye: notebooks, curvas, matrices de confusi√≥n y estudio de ablaci√≥n.

## üì¶ Estructura
```
IA_P3_CIFAR10_Apellido/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ CIFAR10_CNN_Apellido.ipynb   # notebook principal (colab)
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ data_meta.json               # hash y formas de datos
‚îÇ   ‚îú‚îÄ‚îÄ params.yaml                  # hiper-par√°metros
‚îÇ   ‚îú‚îÄ‚îÄ history_*.csv                # curvas de entrenamiento
‚îÇ   ‚îî‚îÄ‚îÄ metrics_*.json               # accuracies finales
‚îú‚îÄ‚îÄ figuras/
‚îÇ   ‚îú‚îÄ‚îÄ muestras_cifar10.png
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix_cnn3.png
‚îÇ   ‚îú‚îÄ‚îÄ errores_cnn3.png
‚îÇ   ‚îî‚îÄ‚îÄ curvas_resumen.png
‚îú‚îÄ‚îÄ env/
‚îÇ   ‚îú‚îÄ‚îÄ ENVIRONMENT.md               # versiones
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt             # pip freeze
‚îî‚îÄ‚îÄ README.md                        # este archivo
```

# A) Conceptos clave ‚Äì Visi√≥n profunda con CNN en CIFAR-10

## Mapa r√°pido del tema
CIFAR-10 son 60 000 im√°genes peque√±as (32√ó32 p√≠xeles y 3 canales de color) divididas en 10 clases: avi√≥n, coche, p√°jaro, gato, venado, perro, rana, caballo, barco y cami√≥n.  
Una CNN supera a una red densa porque **no aplana** la imagen: mantiene la estructura 2D y usa **convoluciones** para detectar bordes, texturas y formas. El **pooling** a√±ade **invarianza a peque√±as traslaciones**: si un gato se mueve unos p√≠xeles, el filtro sigue activ√°ndose. Aplanar los 3 072 p√≠xeles al principio obligar√≠a a la red a aprender de memoria la posici√≥n exacta de cada p√≠xel, con **800 000 par√°metros** solo en la primera capa, y **sensible al ruido de fondo**.

## Convoluci√≥n sin magia
Un filtro (kernel) es una matriz peque√±a (ej. 3√ó3) que se desliza sobre la imagen.  
Hiper-par√°metros: **tama√±o**, **stride** (paso), **padding** (borde) y **canales** (profundidad).  
Ejemplo r√°pido: imagen 5√ó5√ó1, kernel 3√ó3, stride=1, padding=0 ‚Üí salida 3√ó3√ó1.  
Coste: 9√ó3√ó3 = 81 multiplicaciones por canal; si usamos 32 filtros ‚Üí 2 592 ops.

## Pooling y por qu√© importa
**MaxPooling** conserva el valor m√°ximo dentro de una ventana (2√ó2): preserva bordes fuertes y reduce ruido.  
**AveragePooling** suaviza, √∫til en fondos homog√©neos.  
Ambos **dividen a la mitad** la resoluci√≥n, aumentan la **invarianza traslacional** y **disminuyen sobreajuste** al reducir par√°metros.  
Micro-ejemplo: ventana 2√ó2 con valores [[4,2],[3,6]] ‚Üí Max=6, Average=3.75.

## Arquitectura t√≠pica de una CNN simple
Input(32√ó32√ó3)  
‚Üí Conv2D(32 filtros, 3√ó3) + ReLU (detecta bordes)  
‚Üí MaxPool2D(2√ó2) (reduce a 16√ó16)  
‚Üí Conv2D(64 filtros, 3√ó3) + ReLU (formas complejas)  
‚Üí MaxPool2D(2√ó2) (8√ó8)  
‚Üí Flatten (aplanado solo al final)  
‚Üí Dense(128) + ReLU (combinaci√≥n global)  
‚Üí Dropout(0.5) (regularizaci√≥n)  
‚Üí Dense(10, softmax) (probabilidades por clase)

## M√©trica y p√©rdida adecuadas
**P√©rdida**: `categorical_crossentropy` (etiquetas one-hot).  
**M√©trica principal**: `accuracy` (% aciertos).  
**Matriz de confusi√≥n**: muestra qu√© clases se confunden (ej. *cat ‚Üî dog*), √∫til para detectar sesgos o clases dif√≠ciles.



## Normalizaci√≥n y preparaci√≥n de datos
Dividimos por 255.0 para llevar p√≠xeles a [0,1] ‚Üí gradientes estables y LR m√°s altas.  
**Estandarizar por canal** (media 0, desv 1) acelera convergencia en redes profundas o con SGD+momentum.  
Ambas mejoran la **estabilidad num√©rica** y permiten usar **tasas de aprendizaje m√°s grandes** sin divergencia.

## Baseline denso vs CNN
MLP: 3072‚Üí256‚Üí10 ‚Üí ‚âà800 k par√°metros, **sin sesgo espacial**, **sobreajusta** r√°pido ante ruido de fondo.  
CNN: 55 k par√°metros, **sesgo inductivo local** (vecinos ‚Üí patrones), **generaliza** mejor con menos datos y par√°metros.  
La CNN **no aplan** la imagen ‚Üí conserva topolog√≠a y es **m√°s robusta** a peque√±as deformaciones.

## Par√°metros y capacidad
Conv2D:  
`par√°metros = (kernel_h √ó kernel_w √ó canales_entrada + 1) √ó filtros_salida`  
Aumentar **kernel**, **filtros** o **profundidad** ‚Üí m√°s capacidad, m√°s tiempo y riesgo de sobreajuste.  
Profundidad crece capacidad **exponencialmente**; conviene equilibrar con regularizaci√≥n.

## Regularizaci√≥n pr√°ctica
1. **Dropout**: apaga neuronas (0.2-0.5) ‚Üí evita co-adaptaci√≥n.  
2. **L2 weight decay**: penaliza pesos grandes (1e-4) ‚Üí pesos m√°s peque√±os.  
3. **Data Augmentation**: crea variedad artificial ‚Üí robustez.  
4. **Early Stopping**: para cuando val_loss no mejora ‚Üí ahorra tiempo y evita sobreajuste.  
**Combina** las 3 primeras; EarlyStopping siempre obligado.

## Data Augmentation con cabeza
Plan razonable CIFAR-10:  
- Flip horizontal (siempre).  
- Rotaci√≥n ¬±10¬∞.  
- Traslaci√≥n 10 %.  
- Zoom 10 %.  
- Brillo ¬±20 %.  
**L√≠mites**: CIFAR-10 ya es natural ‚Üí evita distorsiones extremas, rotaciones &gt;20¬∞ o cambios de color fuertes.

## Optimizaci√≥n y LR scheduling
**Adam**: adaptativo, r√°pido, pero puede quedarse en m√≠nimos locales.  
**SGD+momentum**: m√°s lento, a veces **mejor generalizaci√≥n**.  
**ReduceLROnPlateau**: baja LR cuando val_loss se estanca 3 √©pocas.  
**CosineDecay**: baja LR suavemente de 0.05 ‚Üí 0 en 30 √©pocas.  
**Se√±al**: val_loss sin mejora ‚Üí bajar LR.

## Curvas de aprendizaje
- **Subajuste**: train/val altas y paralelas ‚Üí aumenta capacidad.  
- **Ajuste saludable**: brecha peque√±a y descendente.  
- **Sobreajuste**: train baja, val sube ‚Üí m√°s regularizaci√≥n.

## Matriz de confusi√≥n y clase dif√≠cil
Pares t√≠picos: *cat ‚Üî dog*, *automobile ‚Üî truck*, *deer ‚Üî horse*.  
Mejoras: m√°s datos de esas clases (augment dirigido), **label smoothing** o **focal loss**.

## Batch size y estabilidad
- **32**: ruido √∫til, generaliza mejor, √©poca lenta.  
- **128**: estable, √©poca r√°pida, pero puede necesitar m√°s √©pocas.  
**Valor inicial en Colab**: 64 (equilibrio tiempo/ruido).

## Buenas pr√°cticas de entrega
1. C√≥digo limpio y comentado.  
2. Semillas fijadas (42).  
3. Logs completos (history.csv, metrics.json).  
4. Curvas y matriz de confusi√≥n.  
5. Tabla comparativa MLP vs CNN.  
6. README con instrucciones de reproducci√≥n.  
7. `requirements.txt` congelado.  
8. Tag de release (`v1.0-P3-CIFAR10_Apellido`).  
9. Informe PDF (2 p√°gs).  
10. 5 hallazgos breves (ej. ‚Äúaugment +2.3 % test acc‚Äù).



## üèÉ‚Äç‚ôÇÔ∏è Uso r√°pido
1. Clona y crea entorno:
```bash
git clone https://github.com/tu-usuario/IA_P3_CIFAR10_Apellido.git
cd IA_P3_CIFAR10_Apellido
python -m venv venv && source venv/bin/activate
pip install -r env/requirements.txt
```
2. Abre el notebook en Colab/Jupyter y ejecuta **Run all**.

## üìä Resultados clave (resumen)
| Modelo                     | test acc | √©pocas | par√°metros | notas |
|----------------------------|----------|--------|------------|-------|
| MLP (baseline)             | 0.XX     | 10     | 800 k      | overfit fuerte |
| CNN-2B                     | 0.XX     | 15     | 55 k       | ‚Äî |
| CNN-2B + L2 + EarlyStop    | 0.XX     | XX     | 55 k       | brecha ‚Üì |
| CNN-3B + augment + sched   | **0.XX** | XX     | 200 k      | **mejor** |
| SGD + CosineDecay          | 0.XX     | XX     | 200 k      | similar, estable |

> Mejora final sobre MLP: **+XX %** con **4√ó menos par√°metros**.

## üîç Ablaci√≥n (contribuci√≥n de cada t√©cnica)
| Variante        | test acc | Œî vs todo |
|-----------------|----------|-----------|
| A todo          | 0.XX     | ‚Äî         |
| B sin augment   | 0.XX     | -0.XX     |
| C sin L2        | 0.XX     | -0.XX     |
| D sin dropout   | 0.XX     | -0.XX     |

**Conclusi√≥n**: *Data augmentation* es la t√©cnica **m√°s influyente**.

## üß™ Reproducibilidad
| elemento        | valor                        |
|-----------------|------------------------------|
| seed            | 42                           |
| TensorFlow      | 2.15.0 (GPU habilitado)      |
| Python          | 3.10.12                      |
| commit          | `abc1234`                    |
| tag             | v1.0-P3-CIFAR10_Apellido     |
| hash datos      | `b5a2c1d8e7f9a1b2` (SHA-256) |

## ‚úçÔ∏è Pr√≥ximos pasos
- Transfer learning con ResNet-20 ‚Üí objetivo 92 %.  
- Label smoothing / MixUp para reducir confusi√≥n *cat ‚Üî dog*.  
- Auto-augment para ganar generalizaci√≥n extra.

